# Research: LLM Post-Processing ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Thai STT

**‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà:** 28 ‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå 2026
**‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠:** ‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏Å‡∏•‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å Whisper STT ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

---

## 1. ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á STT ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

### 1.1 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢

| ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î | ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á | ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏ |
|-----------------|---------|--------|
| **Tone Mark Error** | ‡∏°‡πâ‡∏≤ ‚Üí ‡∏°‡∏≤, ‡∏Ç‡πâ‡∏≤‡∏ß ‚Üí ‡∏Ç‡∏≤‡∏ß | 5 ‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÑ‡∏ó‡∏¢ ‡∏ü‡∏±‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô |
| **Homophone Confusion** | ‡∏Å‡∏≤‡∏ô ‚Üí ‡∏Å‡∏≤‡∏•, ‡πÄ‡∏™‡∏£‡∏¥‡∏° ‚Üí ‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏° | ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡πâ‡∏≠‡∏á ‡∏ï‡πà‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ |
| **Fast Speech** | "‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡∏¢‡∏±‡∏á‡πÑ‡∏á" ‚Üí "‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏∞‡∏¢‡∏±‡∏á‡πÑ‡∏á" | ‡∏ï‡∏±‡∏î‡∏û‡∏¢‡∏≤‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏ö‡∏≤ |
| **Code-switching** | "‡∏ú‡∏° submit ‡∏á‡∏≤‡∏ô ‡πÅ‡∏•‡πâ‡∏ß" ‚Üí "‡∏ú‡∏°‡∏ã‡∏±‡∏ö‡∏°‡∏¥‡∏î‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß" | Thai-English ‡∏õ‡∏ô‡∏Å‡∏±‡∏ô |
| **Filler Words** | "‡πÄ‡∏≠‡πà‡∏≠... ‡∏≠‡πà‡∏≤... ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ú‡∏°‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤" | ‡∏Ñ‡∏≥‡∏≠‡∏∏‡∏ó‡∏≤‡∏ô ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å |
| **No Punctuation** | ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏∏‡∏î‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏•‡∏π‡∏Å‡∏ô‡πâ‡∏≥ | Whisper ‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô |
| **Domain Vocab** | "‡πÑ‡∏Æ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ó‡∏ô‡∏ä‡∏±‡∏ô" ‚Üí "‡πÑ‡∏Æ‡πÄ‡∏õ‡∏≠‡πÄ‡∏ó‡∏ô‡∏ä‡∏±‡πà‡∏ô" | ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á |

### 1.2 ‡∏ó‡∏≥‡πÑ‡∏° WER ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

- ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ word boundary (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥)
- WER ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏¢‡∏±‡∏á‡∏≠‡∏≤‡∏à‡πÑ‡∏î‡πâ 100% error ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ word segmentation ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
- **‚úÖ ‡πÉ‡∏ä‡πâ CER (Character Error Rate)** ‡πÅ‡∏ó‡∏ô ‚Äî ‡∏ß‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞
- ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: "Advocating Character Error Rate for Multilingual ASR Evaluation" (NAACL 2025)

---

## 2. ‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏´‡∏•‡∏±‡∏Å: LLM Post-Processing ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö STT

### 2.1 HyPoradise (NeurIPS 2023) ‚Äî Benchmark ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô

**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:** Dataset ‡πÅ‡∏•‡∏∞ benchmark ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM-based ASR correction

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
- Dataset 316,000+ ‡∏Ñ‡∏π‡πà (N-best hypotheses + correct transcription)
- **Generative correction ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ ranking-based** ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
- LLM ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô N-best list ‡πÄ‡∏•‡∏¢

**ArXiv:** https://arxiv.org/abs/2309.15701

---

### 2.2 Whispering LLaMA (EMNLP 2023) ‚Äî Cross-Modal Approach

**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:** ‡∏£‡∏ß‡∏° Whisper encoder (‡πÄ‡∏™‡∏µ‡∏¢‡∏á) + LLaMA decoder (‡∏†‡∏≤‡∏©‡∏≤) ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
- **37.66% relative WER improvement** ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö N-best rescoring
- ‡πÉ‡∏ä‡πâ acoustic embedding ‡∏à‡∏≤‡∏Å Whisper ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏±‡∏ö LLM correction
- Parameter-efficient fine-tuning

**GitHub:** https://github.com/Srijith-rkr/Whispering-LLaMA

**‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ:**
- ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö Thai: Whisper encoder ‚Üí Typhoon/OpenThaiGPT decoder
- ‡πÅ‡∏ï‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤ ‚Äî ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MVP ‡πÉ‡∏ä‡πâ simple LLM correction ‡∏Å‡πà‡∏≠‡∏ô

---

### 2.3 Task-Activating Prompting (Amazon Science 2023)

**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:** Prompt engineering ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö STT correction

**Key Innovation:** ‡∏ö‡∏≠‡∏Å LLM ‡∏ä‡∏±‡∏î‡πÜ ‡∏ß‡πà‡∏≤ "‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç STT errors" ‚Äî ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏Ñ‡πà‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•

**Prompt pattern ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ú‡∏•:**
```
You are a speech recognition error correction system.
The following text was transcribed from audio and may contain errors.
Correct spelling, grammar, and unclear words while preserving the speaker's intent.
```

**ArXiv:** https://assets.amazon.science/77/26/...

---

### 2.4 ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏£‡∏ß‡∏°

| Approach | WER Improvement |
|----------|----------------|
| N-best rescoring (traditional) | 5-15% |
| LLM prompt correction (zero-shot) | 15-30% |
| LLM + few-shot examples | 25-40% |
| LLM + context window | 35-50% |
| Cross-modal (Whispering LLaMA) | 37-50%+ |

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤ CER improvement: 40-60%** ‡∏à‡∏≤‡∏Å STT-only ‚Üí STT + LLM

---

## 3. Context Window Strategy

### 3.1 ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£: ‡∏ó‡∏≥‡πÑ‡∏° Context ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏î‡πâ

```
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå

‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ context:
   STT: "‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏ô 140 ‡∏ï‡πà‡∏≠ 90 ‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ ‡πÑ‡∏Æ‡πÄ‡∏õ‡∏≠‡πÄ‡∏ó‡∏ô‡∏ä‡∏±‡πà‡∏ô"
   ‚Üí LLM ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ domain ‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå

‚úÖ ‡∏°‡∏µ context (2-3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤):
   Context: "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢... ‡πÇ‡∏£‡∏Ñ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏≠‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏î..."
   STT: "‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏ô 140 ‡∏ï‡πà‡∏≠ 90 ‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ ‡πÑ‡∏Æ‡πÄ‡∏õ‡∏≠‡πÄ‡∏ó‡∏ô‡∏ä‡∏±‡πà‡∏ô"
   ‚Üí LLM ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô: "‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏ô 140/90 ‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ Hypertension"
```

### 3.2 Sliding Window Approach (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)

```
Session: [s1][s2][s3][s4][s5]...
                   ‚Üë
               ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô

Context ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM = [s3][s4] (2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤)
```

**‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏û‡∏ö‡∏ß‡πà‡∏≤:**
- **2-3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤** ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (diminishing returns ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô)
- ‡πÄ‡∏Å‡πá‡∏ö context ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏ï‡∏•‡∏≠‡∏î session
- ‡∏•‡πâ‡∏≤‡∏á context ‡πÄ‡∏°‡∏∑‡πà‡∏≠ user ‡πÄ‡∏£‡∏¥‡πà‡∏° topic ‡πÉ‡∏´‡∏°‡πà (‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å pause ‡∏¢‡∏≤‡∏ß ‡∏´‡∏£‡∏∑‡∏≠ explicit command)

### 3.3 Domain Detection

‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö domain ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å context ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏™‡πà‡πÉ‡∏ô prompt:

```rust
enum Domain {
    Medical,    // ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å: ‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢, ‡∏¢‡∏≤, ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£, ‡πÇ‡∏£‡∏Ñ
    Legal,      // ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å: ‡∏™‡∏±‡∏ç‡∏ç‡∏≤, ‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢, ‡∏Ç‡πâ‡∏≠‡∏û‡∏¥‡∏û‡∏≤‡∏ó
    Technical,  // ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å: code, function, deploy, server
    Casual,     // default
}
```

---

## 4. Local LLM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Thai (Lightweight)

### 4.1 ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å

| Model | ‡∏Ç‡∏ô‡∏≤‡∏î | Thai Support | RAM (Q4) | Speed (CPU) | ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö |
|-------|------|-------------|---------|------------|-------------|
| **Typhoon2-Qwen2.5-7B** | 7B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Thai-specialized | ~5GB | ‡∏ä‡πâ‡∏≤‡∏ö‡∏ô CPU | GPU users |
| **OpenThaiGPT 1.5-7B** | 7B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Thai-native | ~5GB | ‡∏ä‡πâ‡∏≤‡∏ö‡∏ô CPU | GPU users |
| **Qwen2.5-3B** | 3B | ‚≠ê‚≠ê‚≠ê‚≠ê (29 languages) | ~2.5GB | ‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ | **Balance** ‚úÖ |
| **Llama 3.2-3B** | 3B | ‚≠ê‚≠ê‚≠ê‚≠ê Thai confirmed | ~2.5GB | ‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ | Alternative |
| **Qwen2.5-1.5B** | 1.5B | ‚≠ê‚≠ê‚≠ê | ~1.2GB | ‚úÖ ‡πÄ‡∏£‡πá‡∏ß | Low-end PC |
| **Gemma 3-1B** | 1B | ‚≠ê‚≠ê‚≠ê (140 languages) | ~0.9GB | ‚úÖ ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å | Minimal RAM |
| **phi-4-mini (3.8B)** | 3.8B | ‚≠ê‚≠ê‚≠ê | ~3GB | ‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ | Reasoning tasks |

### 4.2 ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ

```
‡∏ï‡∏≤‡∏°‡∏™‡πÄ‡∏õ‡∏Ñ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á:

CPU ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (RAM < 8GB):
  ‚Üí Qwen2.5-1.5B GGUF Q4 (‡πÄ‡∏£‡πá‡∏ß, Thai ‡πÇ‡∏≠‡πÄ‡∏Ñ)

CPU ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (RAM 8-16GB):
  ‚Üí Qwen2.5-3B GGUF Q4 (balance ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ‚úÖ

‡∏°‡∏µ GPU (VRAM 6GB+):
  ‚Üí Typhoon2-Qwen2.5-7B (Thai ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î)
  ‚Üí ‡∏´‡∏£‡∏∑‡∏≠ OpenThaiGPT 1.5-7B
```

### 4.3 Rust Integration

```toml
[dependencies]
# llama.cpp Rust bindings
llama_cpp = "0.3"

# ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ Ollama REST API (‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤)
reqwest = { version = "0.12", features = ["json"] }
```

**‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Integration:**

**Option A: llama.cpp binding (in-process)**
- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ô external service
- ‡πÉ‡∏ä‡πâ `llama_cpp` crate
- Memory ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô process ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

**Option B: Ollama REST API (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MVP)**
- User ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Ollama ‡πÅ‡∏¢‡∏Å
- Call `http://localhost:11434/api/generate`
- ‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤, flexible ‡∏Å‡∏ß‡πà‡∏≤
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏™‡∏•‡∏±‡∏ö model ‡πÑ‡∏î‡πâ

---

## 5. System Architecture: STT + LLM Pipeline

### 5.1 Flow ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô

```
[Push-to-Talk]
     ‚îÇ
     ‚ñº
[cpal: Audio Capture] ‚îÄ‚îÄ‚îÄ‚îÄ 16kHz mono f32
     ‚îÇ
     ‚ñº
[VAD: Silero] ‚îÄ‚îÄ‚îÄ‚îÄ ‡∏ï‡∏±‡∏î silence ‡∏≠‡∏≠‡∏Å
     ‚îÇ
     ‚ñº
[whisper-rs: STT] ‚îÄ‚îÄ‚îÄ‚îÄ ~5-15s
     ‚îÇ raw_text (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)
     ‚ñº
[Context Manager]
  ‚îú‚îÄ‚îÄ rolling_window: Vec<String> (2-3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
  ‚îú‚îÄ‚îÄ domain: Domain (Medical/Legal/Tech/Casual)
  ‚îî‚îÄ‚îÄ user_vocab: HashMap<String, String>
     ‚îÇ
     ‚ñº
[LLM Correction] ‚îÄ‚îÄ‚îÄ‚îÄ ~1-3s (Qwen 3B CPU)
  Input: raw_text + context + domain + user_vocab
  Output: corrected_text
     ‚îÇ
     ‚ñº
[Text Injection]
  arboard (clipboard) + enigo (Ctrl+V)
     ‚îÇ
     ‚ñº
[Active Window]
```

### 5.2 Prompt Engineering ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

```
SYSTEM PROMPT:
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å Speech-to-Text ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á ‡πÇ‡∏î‡∏¢‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏î‡∏¥‡∏°

‡∏Å‡∏é:
1. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡∏ú‡∏¥‡∏î, ‡∏Ñ‡∏≥‡∏û‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á)
2. ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏≠‡∏∏‡∏ó‡∏≤‡∏ô (‡πÄ‡∏≠‡πà‡∏≠, ‡∏≠‡πà‡∏≤, ‡∏≠‡πà‡∏≤‡∏ô‡∏∞) ‡∏≠‡∏≠‡∏Å
3. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
4. ‡∏£‡∏±‡∏Å‡∏©‡∏≤ code/‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ ‡πÑ‡∏°‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
5. ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢

---
USER PROMPT:
domain: {domain}
‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©: {user_vocab}

‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤:
{context_window}

‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏≠‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á:
{raw_text}
```

### 5.3 Few-Shot Examples ‡πÉ‡∏ô Prompt (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)

```
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:
Input:  "‡πÄ‡∏≠‡πà‡∏≠ ‡∏ú‡∏° ‡πÄ‡∏™‡∏£‡πá‡∏à ‡∏á‡∏≤‡∏ô ‡πÅ‡∏•‡πâ‡∏ß ‡∏ô‡∏∞ ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏à‡∏∞ ‡∏™‡πà‡∏á ‡πÉ‡∏´‡πâ ‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ"
Output: "‡∏ú‡∏°‡πÄ‡∏™‡∏£‡πá‡∏à‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏∞‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ"

Input:  "‡πÑ‡∏ü‡∏•‡πå ‡∏°‡∏±‡∏ô ‡πÑ‡∏°‡πà ‡πÇ‡∏´‡∏•‡∏î ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ network connection ‡∏°‡∏±‡∏ô drop"
Output: "‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏û‡∏£‡∏≤‡∏∞ network connection ‡∏°‡∏±‡∏ô drop"
```

---

## 6. Context Window Design (‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î)

### 6.1 Context Manager Structure

```rust
struct ContextManager {
    // Rolling window: ‡πÄ‡∏Å‡πá‡∏ö N ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
    history: VecDeque<String>,
    max_history: usize,      // ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: 3

    // Domain detection
    domain: Domain,
    domain_keywords: HashMap<Domain, Vec<&'static str>>,

    // User custom vocabulary
    user_vocab: HashMap<String, String>,  // ‡∏ú‡∏¥‡∏î ‚Üí ‡∏ñ‡∏π‡∏Å

    // Session metadata
    session_start: Instant,
    last_utterance: Instant,
    silence_threshold: Duration,  // ‡∏ñ‡πâ‡∏≤ pause > 30s ‚Üí ‡∏•‡πâ‡∏≤‡∏á context
}
```

### 6.2 Domain Detection Algorithm

```rust
fn detect_domain(text: &str, history: &[String]) -> Domain {
    let all_text = history.join(" ") + " " + text;

    let scores = [
        (Domain::Medical, vec!["‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢","‡∏¢‡∏≤","‡∏≠‡∏≤‡∏Å‡∏≤‡∏£","‡πÇ‡∏£‡∏Ñ","‡πÅ‡∏û‡∏ó‡∏¢‡πå","‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢"]),
        (Domain::Legal,   vec!["‡∏™‡∏±‡∏ç‡∏ç‡∏≤","‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢","‡∏Ç‡πâ‡∏≠‡∏û‡∏¥‡∏û‡∏≤‡∏ó","‡∏Ñ‡∏î‡∏µ","‡∏®‡∏≤‡∏•"]),
        (Domain::Technical, vec!["code","function","server","deploy","bug","database"]),
    ];

    // ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ domain ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å domain ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    // ‡∏ñ‡πâ‡∏≤‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ threshold ‚Üí Casual
}
```

### 6.3 User Vocabulary Learning

```rust
// ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô local file: ~/.config/thai-vtt/vocab.json
{
    "‡πÑ‡∏Æ‡πÄ‡∏õ‡∏≠‡πÄ‡∏ó‡∏ô‡∏ä‡∏±‡πà‡∏ô": "Hypertension",
    "‡∏ã‡∏µ‡∏û‡∏µ‡∏¢‡∏π": "CPU",
    "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤": "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ABC ‡∏à‡∏≥‡∏Å‡∏±‡∏î",
    "‡∏ö‡∏≠‡∏™": "‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ä‡∏≤‡∏¢"
}
```

User ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:
- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ú‡πà‡∏≤‡∏ô UI ‡∏Ç‡∏≠‡∏á widget
- ‡∏£‡∏∞‡∏ö‡∏ö auto-learn ‡∏à‡∏≤‡∏Å correction history (future feature)

---

## 7. Performance Budget

### 7.1 Latency ‡∏ï‡πà‡∏≠ utterance (10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡πÄ‡∏™‡∏µ‡∏¢‡∏á)

| ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô | CPU (no GPU) | GPU |
|---------|-------------|-----|
| Audio capture | ~0ms | ~0ms |
| VAD (Silero) | ~50ms | ~20ms |
| Whisper STT (medium) | **5-15s** | **1-3s** |
| LLM Correction (Qwen 3B) | **2-5s** | **0.5-1s** |
| Text inject | ~10ms | ~10ms |
| **‡∏£‡∏ß‡∏°** | **~8-20s** | **~2-4s** |

### 7.2 ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á UX ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏≠

```
Strategy: ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏ö‡∏ö 2 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

1. ‡πÅ‡∏™‡∏î‡∏á raw STT ‡∏Å‡πà‡∏≠‡∏ô (‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á Whisper ‡πÄ‡∏™‡∏£‡πá‡∏à)
   ‚Üí ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏•‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÅ‡∏°‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà clean

2. LLM ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô background
   ‚Üí ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô widget
   ‚Üí ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ corrected version

UI:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üî¥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ü‡∏±‡∏á...              ‚îÇ
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ
‚îÇ [raw] ‡∏ú‡∏°‡πÄ‡∏™‡∏£‡πá‡∏à‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö  ‚îÇ
‚îÇ [‚ú®]  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏Å‡∏•‡∏≤‡∏Ñ‡∏≥...         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 7.3 Memory Usage

| Component | RAM |
|-----------|-----|
| Whisper Medium | ~800MB |
| Qwen 3B GGUF Q4 | ~2.5GB |
| App overhead | ~50MB |
| **‡∏£‡∏ß‡∏°** | **~3.5GB** |

> **‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:** ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á RAM ‡∏ô‡πâ‡∏≠‡∏¢ ‡πÉ‡∏ä‡πâ Whisper Small + Qwen 1.5B ‚Üí ‡∏£‡∏ß‡∏° ~1.5GB

---

## 8. ‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö

### Mode 1: Fast Mode (STT only)
- ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô LLM
- ‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î, latency ‡∏ï‡πà‡∏≥
- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö casual use

### Mode 2: Standard Mode (STT + LLM correction)
- ‡∏ú‡πà‡∏≤‡∏ô LLM ‡πÄ‡∏Å‡∏•‡∏≤‡∏Ñ‡∏≥ + ‡∏ï‡∏±‡∏î filler words + ‡πÄ‡∏û‡∏¥‡πà‡∏° punctuation
- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

### Mode 3: Context Mode (STT + LLM + rolling context)
- ‡πÄ‡∏û‡∏¥‡πà‡∏° context window
- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö long-form dictation, writing
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î

---

## 9. Papers ‡πÅ‡∏•‡∏∞ References

| Paper | Venue | Key Finding | Link |
|-------|-------|-------------|------|
| HyPoradise | NeurIPS 2023 | Generative > Ranking for ASR correction | https://arxiv.org/abs/2309.15701 |
| Whispering LLaMA | EMNLP 2023 | 37.66% WER improvement, cross-modal | https://arxiv.org/abs/2310.06434 |
| Task-Activating Prompting | Amazon 2023 | Better prompts for STT correction | assets.amazon.science |
| Thai Tone in ASR | ACL | Tone marks critical for Thai STT | Y14-1023 |
| CER for Multilingual | NAACL 2025 | CER ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ WER ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Thai | NAACL Findings 2025 |
| Typhoon2 | SCB10X 2024 | Thai-specialized LLM, 7B | https://huggingface.co/scb10x/typhoon2-qwen2.5-7b-instruct |
| OpenThaiGPT 1.5 | AIEAT 2024 | Thai-native 7B, 131K context | https://huggingface.co/openthaigpt/openthaigpt1.5-7b-instruct |

---

## 10. ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MVP

### Stack ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å research ‡πÄ‡∏î‡∏¥‡∏°)

```
‡πÄ‡∏î‡∏¥‡∏°:  Whisper STT ‚Üí Text Inject
‡πÉ‡∏´‡∏°‡πà: Whisper STT ‚Üí LLM Correction (Qwen 3B / Ollama) ‚Üí Text Inject
                          ‚Üë
                   Context Manager
                   (rolling window + domain + user vocab)
```

### Priority ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤

**Phase 1 (MVP):**
- Whisper STT ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡∏°‡∏µ LLM)
- ‡∏ó‡∏î‡∏™‡∏≠‡∏ö pipeline ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

**Phase 2 (LLM Basic):**
- ‡πÄ‡∏û‡∏¥‡πà‡∏° Ollama integration (Qwen 3B)
- Basic filler word removal + punctuation
- Zero-shot correction

**Phase 3 (Context):**
- Rolling window context (3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ)
- Domain detection
- User vocabulary

**Phase 4 (Polish):**
- Few-shot examples ‡πÉ‡∏ô prompt
- Model selection UI
- Fast/Standard/Context mode toggle
- Auto-learn vocabulary
